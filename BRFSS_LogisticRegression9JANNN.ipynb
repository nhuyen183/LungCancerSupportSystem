{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1usnFJ9lgDS3xPq/ObNP8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhuyen183/LungCancerSupportSystem/blob/master/BRFSS_LogisticRegression9JANNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B7eaubtZ_Kj",
        "outputId": "ceaaa6bd-2dec-4677-f964-61edc75a3796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.12.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.5.1.post0\n"
          ]
        }
      ],
      "source": [
        "#@title Import Libraries { vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "import sqlite3\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.utils.validation import check_is_fitted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title df_structure { display-mode: \"both\" }\n",
        "\n",
        "from  glob import glob\n",
        "filenames = glob('./part*.csv')\n",
        "appended_data = []  # create a list\n",
        "for f in filenames:\n",
        "  df = pd.read_csv(f, index_col=None)\n",
        "  appended_data.append(df)  # append to the list\n",
        "df_structure = pd.concat(appended_data, axis=0)\n",
        "df_structure.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "6cwVYFkxaMAu",
        "outputId": "d1bdd34d-0dca-4d78-8337-456958050cbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-879251c42305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mappended_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# append to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappended_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_structure"
      ],
      "metadata": {
        "id": "faazJz6NbUhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HasLungCancer { display-mode: \"both\" }\n",
        "print (df[\"HasLungCancer\"].value_counts())"
      ],
      "metadata": {
        "id": "oWwUzqxZbZpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "LDHYgfI1bpAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create correlation matrix\n",
        "correlation = df.select_dtypes(\"number\").drop(columns=\"HasLungCancer\").corr()\n",
        "\n",
        "# Plot heatmap of `correlation`\n",
        "sns.heatmap(correlation);"
      ],
      "metadata": {
        "id": "E8l_qP1DbwRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "majority_class_prop, minority_class_prop = y.value_counts(normalize=True)\n",
        "print(majority_class_prop, minority_class_prop)"
      ],
      "metadata": {
        "id": "YGHcp5Amb8nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='HasLungCancer').values\n",
        "y = df['HasLungCancer'].values\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# transform the dataset\n",
        "sm = SMOTE(random_state=42)\n",
        "X, y = sm.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "UNvG_jHXvAzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "99ihsBYBu3ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# Initializing the ANN\n",
        "ann = tf.keras.models.Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units=200, input_dim=13, activation='relu'))\n",
        "# Adding the second hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Part 3 - Training the ANN\n",
        "\n",
        "# Compiling the ANN\n",
        "ann.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy', 'binary_crossentropy'])\n",
        "print(ann.summary())\n",
        "\n",
        "# Training the ANN on the Training set\n",
        "history = ann.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100, epochs =10, verbose=1)"
      ],
      "metadata": {
        "id": "28xJb_egjNYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['binary_crossentropy'])\n",
        "plt.title('Loss Function Over Epochs')\n",
        "plt.ylabel('loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eIYNv9quBTD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict_proba(X_test)\n",
        "\n",
        "#y_predInverse = sc.inverse_transform(y_pred)\n",
        "#y_testInverse = sc.inverse_transform(y_test)"
      ],
      "metadata": {
        "id": "Ih6eEjYnBW2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildModel(optimizer):\n",
        "    # Initialising the ANN\n",
        "    classifier = Sequential()\n",
        "    \n",
        "    # Adding the input layer and the first hidden layer\n",
        "    classifier.add(Dense(units = 200, input_dim = 13, activation = 'relu'))\n",
        "    \n",
        "    # Adding the second hidden layer\n",
        "    classifier.add(Dense(units = 100, activation = 'relu'))\n",
        "    \n",
        "    # Adding the output layer\n",
        "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "    \n",
        "    # Compiling the ANN\n",
        "    classifier.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['mean_absolute_error', 'binary_crossentropy'])\n",
        "    \n",
        "    return classifier"
      ],
      "metadata": {
        "id": "BXtdVBF0EGe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV \n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "classifier = KerasRegressor(build_fn = buildModel)\n",
        "#What hyperparameter we want to play with\n",
        "parameters = {'batch_size': [13, 26, 39, 50],\n",
        "              'epochs': [40, 50],\n",
        "              'optimizer': ['adam', 'binary_crossentropy']}\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'neg_mean_absolute_error',\n",
        "                           cv = 5)\n",
        "grid_search = grid_search.fit(X_train, y_train, verbose = 0)"
      ],
      "metadata": {
        "id": "Ut3C7Pm1FIaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters: \" + str(best_parameters))"
      ],
      "metadata": {
        "id": "AC49nkrJKsra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0YM0s_3H7yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "classifier = make_pipeline(MinMaxScaler(), LogisticRegression(max_iter=2000))\n",
        "\n",
        "# Fit model to training data\n",
        "history = classifier.fit(X_train, y_train)\n",
        "print(classifier)"
      ],
      "metadata": {
        "id": "RVOyeRubsPQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate the model { display-mode: \"both\" }\n",
        "acc_train = accuracy_score(y_train, classifier.predict(X_train))\n",
        "acc_test = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Training Accuracy:\", round(acc_train, 4))\n",
        "print(\"Test Accuracy:\", round(acc_test, 4))"
      ],
      "metadata": {
        "id": "t1057yFXvT-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#acc_baseline = y_train.value_counts(normalize=True).max()\n",
        "#print(\"Baseline Accuracy:\", round(acc_baseline, 2))\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "id": "HSMZD7BziiIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "metrics = classification_report(y_test, y_pred)\n",
        "print(metrics)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "OEiEggZNsKKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ax = sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['False','True'])\n",
        "ax.yaxis.set_ticklabels(['False','True'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uoEJfOnVspOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.named_steps[\"logisticregression\"].intercept_[0]"
      ],
      "metadata": {
        "id": "fm_LgSKk6VoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.named_steps[\"logisticregression\"].coef_[0]"
      ],
      "metadata": {
        "id": "RCjkDziQ6eCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = classifier.named_steps[\"minmaxscaler\"].get_feature_names_out(['Gender','Age65','BMI','GeneralHealth','Smoked100','SmokerStatus','FirstSmokedAge','LastSmokedAge','AvgNumCigADay','HasCTScan','StopSmoking','HasAsthma','HasChronicDisease'])\n",
        "importances = classifier.named_steps[\"logisticregression\"].coef_[0]\n",
        "print(features, importances)"
      ],
      "metadata": {
        "id": "P6Ih0JMTrkWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "odds_ratios = pd.Series(np.exp(importances),index=features).sort_values()\n",
        "odds_ratios"
      ],
      "metadata": {
        "id": "QVpK3ghTrn6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Horizontal bar chart, five largest coefficients\n",
        "odds_ratios.plot(kind=\"barh\")\n",
        "plt.xlabel(\"Odds Ratio\");"
      ],
      "metadata": {
        "id": "RU6NKCO7rq9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = history.history['accuracy']\n",
        "\n",
        "loss_val = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1,51)\n",
        "\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "\n",
        "plt.title('Training and Validation accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JhRxWi8o-Oud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows the features that are least likely to be in a building that is severely damaged."
      ],
      "metadata": {
        "id": "IfGaKlc2CIsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = history.history['train_loss']\n",
        "\n",
        "loss_val = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,35)\n",
        "\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "\n",
        "plt.title('Training and Validation loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgQzKBZkiyU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotGraph(y_test,y_pred,classifier):\n",
        "    if max(y_test) >= max(y_pred):\n",
        "        my_range = int(max(y_test))\n",
        "    else:\n",
        "        my_range = int(max(y_pred))\n",
        "    plt.scatter(range(len(y_test)), y_test, color='blue')\n",
        "    plt.scatter(range(len(y_pred)), y_pred, color='red')\n",
        "    plt.title(classifier)\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "plotGraph(y_test, y_pred, \"test\")"
      ],
      "metadata": {
        "id": "w4zP9y4oD2N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kehtu2voH4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = history.history['loss']\n",
        "\n",
        "loss_val = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,51)\n",
        "\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "\n",
        "plt.title('Training and Validation loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UpAZ58Ufj2wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train = history.history['accuracy']\n",
        "\n",
        "loss_val = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1,51)\n",
        "\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "\n",
        "plt.title('Training and Validation accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9jcv84n7krDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPbBx8BG-LMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}